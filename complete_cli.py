#!/usr/bin/env python3
"""
Complete Ultimate Local AI CLI - Working Version
This version connects to actual Ollama models for real AI functionality
"""

import typer
import asyncio
import sys
import os
import subprocess
import json
from rich.console import Console
from rich.panel import Panel
from rich.text import Text
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.live import Live
from rich.prompt import Prompt
import time
import requests
from typing import Optional

app = typer.Typer(
    name="ultimate-ai",
    help="Ultimate Local AI - Complete CLI with Real AI Models",
    rich_markup_mode="rich"
)

console = Console()

# Ollama API base URL
OLLAMA_BASE_URL = "http://localhost:11434"

def check_ollama_running():
    """Check if Ollama is running"""
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
        return response.status_code == 200
    except:
        return False

def get_available_models():
    """Get list of available models from Ollama"""
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
        if response.status_code == 200:
            data = response.json()
            return [model["name"] for model in data.get("models", [])]
        return []
    except:
        return []

def chat_with_ollama(message: str, model: str = "qwen3:32b", system: str = None):
    """Chat with Ollama model"""
    try:
        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": system or "You are Ultimate Local AI, an advanced AI assistant running locally with focus on privacy and intelligence."},
                {"role": "user", "content": message}
            ],
            "stream": False
        }
        
        response = requests.post(f"{OLLAMA_BASE_URL}/api/chat", json=payload, timeout=120)
        
        if response.status_code == 200:
            data = response.json()
            return data["message"]["content"]
        else:
            return f"Error: {response.status_code} - {response.text}"
    except Exception as e:
        return f"Error communicating with Ollama: {e}"

def stream_chat_with_ollama(message: str, model: str = "qwen3:32b", system: str = None):
    """Stream chat with Ollama model"""
    try:
        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": system or "You are Ultimate Local AI, an advanced AI assistant."},
                {"role": "user", "content": message}
            ],
            "stream": True
        }
        
        response = requests.post(f"{OLLAMA_BASE_URL}/api/chat", json=payload, stream=True, timeout=60)
        
        if response.status_code == 200:
            full_response = ""
            for line in response.iter_lines():
                if line:
                    try:
                        data = json.loads(line)
                        if "message" in data and "content" in data["message"]:
                            content = data["message"]["content"]
                            full_response += content
                            yield content
                        if data.get("done", False):
                            break
                    except json.JSONDecodeError:
                        continue
            return full_response
        else:
            yield f"Error: {response.status_code}"
    except Exception as e:
        yield f"Error: {e}"

@app.command()
def chat(
    message: Optional[str] = typer.Argument(None, help="Message to send to the AI"),
    interactive: bool = typer.Option(False, "--interactive", "-i", help="Start interactive mode"),
    model: str = typer.Option("qwen3:32b", "--model", "-m", help="Model to use"),
    system: str = typer.Option(None, "--system", "-s", help="System prompt"),
    stream: bool = typer.Option(True, "--stream/--no-stream", help="Stream responses"),
) -> None:
    """
    Chat with Ultimate Local AI using real AI models
    
    Examples:
      ultimate-ai chat "Hello!"
      ultimate-ai chat --interactive
      ultimate-ai chat "Explain quantum computing" --model qwen2.5:14b
    """
    
    console.print(Panel(
        Text("ü§ñ Ultimate Local AI - Real AI Chat", justify="center"),
        style="bold cyan",
        border_style="cyan"
    ))
    
    # Check if Ollama is running
    if not check_ollama_running():
        console.print("[red]‚ùå Ollama is not running. Please start it with: brew services start ollama[/red]")
        console.print("[yellow]üí° Or run: ollama serve[/yellow]")
        return
    
    # Check if model is available
    available_models = get_available_models()
    if model not in available_models:
        console.print(f"[red]‚ùå Model '{model}' not found.[/red]")
        if available_models:
            console.print(f"[yellow]Available models: {', '.join(available_models)}[/yellow]")
            console.print(f"[yellow]üí° Download with: ollama pull {model}[/yellow]")
        else:
            console.print("[yellow]üí° No models found. Download one with: ollama pull qwen2.5:7b[/yellow]")
        return
    
    console.print(f"[green]‚úÖ Using model: {model}[/green]")
    if system:
        console.print(f"[dim]System: {system}[/dim]")
    console.print()
    
    if interactive or not message:
        # Interactive mode
        console.print("[cyan]Starting interactive chat mode...[/cyan]")
        console.print("[dim]Type 'exit', 'quit', or 'bye' to end the conversation[/dim]")
        console.print("[dim]Type '/models' to see available models[/dim]")
        console.print("[dim]Type '/system <prompt>' to change system prompt[/dim]")
        console.print()
        
        conversation_history = []
        current_system = system or "You are Ultimate Local AI, an advanced AI assistant running locally with focus on privacy, intelligence, and helpful responses."
        
        while True:
            try:
                if not message:
                    message = Prompt.ask("\n[bold green]You[/bold green]")
                
                if message.lower() in ['exit', 'quit', 'bye', 'q']:
                    console.print("\n[yellow]Thanks for chatting! Goodbye! üëã[/yellow]")
                    break
                
                if message.startswith('/models'):
                    models_table = Table(show_header=True, header_style="bold green")
                    models_table.add_column("Model", style="cyan")
                    models_table.add_column("Status", style="green")
                    
                    for m in available_models:
                        status = "‚úÖ Active" if m == model else "üìã Available"
                        models_table.add_row(m, status)
                    
                    console.print(models_table)
                    message = None
                    continue
                
                if message.startswith('/system '):
                    current_system = message[8:]
                    console.print(f"[green]‚úÖ System prompt updated[/green]")
                    message = None
                    continue
                
                # Show thinking indicator
                with console.status("[bold blue]ü§î AI is thinking...", spinner="dots"):
                    if stream:
                        console.print(f"\n[bold blue]ü§ñ Ultimate AI[/bold blue]: ", end="")
                        response_text = ""
                        for chunk in stream_chat_with_ollama(message, model, current_system):
                            print(chunk, end="", flush=True)
                            response_text += chunk
                        console.print()
                    else:
                        response = chat_with_ollama(message, model, current_system)
                        console.print(f"\n[bold blue]ü§ñ Ultimate AI[/bold blue]: {response}")
                
                message = None  # Reset for next iteration
                
            except (KeyboardInterrupt, EOFError):
                console.print("\n[yellow]Chat ended. Goodbye! üëã[/yellow]")
                break
    else:
        # Single message mode
        with console.status("[bold blue]ü§î AI is thinking...", spinner="dots"):
            if stream:
                console.print(f"[bold blue]ü§ñ Ultimate AI[/bold blue]: ", end="")
                for chunk in stream_chat_with_ollama(message, model, system):
                    print(chunk, end="", flush=True)
                console.print()
            else:
                response = chat_with_ollama(message, model, system)
                console.print(f"[bold blue]ü§ñ Ultimate AI[/bold blue]: {response}")

@app.command()
def reasoning(
    problem: str = typer.Argument(..., help="Problem to solve with advanced reasoning"),
    model: str = typer.Option("qwen3:32b", "--model", "-m", help="Model to use"),
    show_steps: bool = typer.Option(True, "--steps/--no-steps", help="Show reasoning steps"),
) -> None:
    """
    Use advanced reasoning capabilities to solve complex problems
    
    Example:
      ultimate-ai reasoning "How can I optimize a slow database query?"
    """
    
    console.print(Panel(
        Text("üß† Advanced Reasoning Engine", justify="center"),
        style="bold magenta",
        border_style="magenta"
    ))
    
    # Check if Ollama is running
    if not check_ollama_running():
        console.print("[red]‚ùå Ollama is not running. Please start it first.[/red]")
        return
    
    # Check if model is available
    available_models = get_available_models()
    if model not in available_models:
        console.print(f"[red]‚ùå Model '{model}' not found. Use --model to specify an available model.[/red]")
        return
    
    console.print(f"\n[bold yellow]Problem:[/bold yellow] {problem}\n")
    
    # Advanced reasoning system prompt
    reasoning_system = """You are an advanced reasoning AI assistant. When solving problems:

1. **Analysis**: Break down the problem into components
2. **Reasoning**: Apply step-by-step logical thinking  
3. **Solution**: Provide a clear, actionable solution
4. **Confidence**: Indicate your confidence level (0-100%)

Use this format:
üîç **Analysis**: [Your analysis]
üß† **Reasoning**: [Step-by-step reasoning]
üí° **Solution**: [Clear solution]
‚úÖ **Confidence**: [0-100%]

Be thorough, logical, and practical."""
    
    if show_steps:
        # Show thinking process
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console
        ) as progress:
            steps = [
                "üîç Analyzing problem complexity...",
                "üß© Breaking down into components...",
                "ü§î Applying reasoning patterns...",
                "üí° Synthesizing solution...",
                "‚úÖ Validating approach..."
            ]
            
            task = progress.add_task("", total=None)
            for step in steps:
                progress.update(task, description=step)
                time.sleep(0.8)
    
    # Generate reasoning response
    with console.status("[bold blue]üß† Deep reasoning in progress...", spinner="dots"):
        response = chat_with_ollama(problem, model, reasoning_system)
        console.print(f"\n[bold blue]üß† Reasoning Result[/bold blue]:\n{response}\n")

@app.command()
def models() -> None:
    """List available AI models and their status"""
    
    console.print(Panel(
        Text("ü§ñ Available AI Models", justify="center"),
        style="bold green",
        border_style="green"
    ))
    
    if not check_ollama_running():
        console.print("[red]‚ùå Ollama is not running. Please start it first.[/red]")
        return
    
    available_models = get_available_models()
    
    if not available_models:
        console.print("[yellow]No models found. Download models with:[/yellow]")
        console.print("  ‚Ä¢ [cyan]ollama pull qwen3:32b[/cyan] (32B - Best performance)")
        console.print("  ‚Ä¢ [cyan]ollama pull qwen2.5:14b[/cyan] (14B - Good balance)")
        console.print("  ‚Ä¢ [cyan]ollama pull qwen2.5:7b[/cyan] (7B - Faster responses)")
        return
    
    models_table = Table(show_header=True, header_style="bold green")
    models_table.add_column("Model", style="cyan", width=25)
    models_table.add_column("Status", style="green", width=15)
    models_table.add_column("Use Case", style="yellow", width=35)
    
    model_info = {
        "qwen3:32b": "Advanced reasoning & complex tasks",
        "qwen2.5-coder:32b": "Code generation & debugging", 
        "qwen2.5:14b": "General conversation & analysis",
        "qwen2.5:7b": "Quick responses & lightweight tasks",
        "llava:7b": "Vision & image analysis",
        "mistral:7b": "Fast general purpose",
        "codellama:13b": "Code-focused tasks"
    }
    
    for model in available_models:
        use_case = model_info.get(model, "General purpose AI model")
        models_table.add_row(model, "‚úÖ Available", use_case)
    
    console.print(models_table)
    console.print(f"\n[dim]Total models: {len(available_models)}[/dim]")
    console.print("[dim]Use --model <name> to specify a model[/dim]")

@app.command()
def pull(
    model: str = typer.Argument(..., help="Model name to download"),
    force: bool = typer.Option(False, "--force", "-f", help="Force re-download"),
) -> None:
    """Download and install an AI model"""
    
    console.print(f"[cyan]üì• Downloading model: {model}[/cyan]")
    
    if not check_ollama_running():
        console.print("[red]‚ùå Ollama is not running. Please start it first.[/red]")
        return
    
    # Run ollama pull command
    try:
        cmd = ["ollama", "pull", model]
        if force:
            # Ollama doesn't have a force flag, but we can delete and re-pull
            subprocess.run(["ollama", "rm", model], capture_output=True)
        
        console.print(f"[yellow]This may take several minutes depending on model size...[/yellow]")
        
        # Run the command and show output
        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
        
        for line in process.stdout:
            console.print(line.strip())
        
        process.wait()
        
        if process.returncode == 0:
            console.print(f"[green]‚úÖ Model {model} downloaded successfully![/green]")
            console.print(f"[dim]You can now use it with: ultimate-ai chat --model {model}[/dim]")
        else:
            console.print(f"[red]‚ùå Failed to download {model}[/red]")
            
    except FileNotFoundError:
        console.print("[red]‚ùå Ollama not found. Please install it first: brew install ollama[/red]")
    except Exception as e:
        console.print(f"[red]‚ùå Error downloading model: {e}[/red]")

@app.command()
def info() -> None:
    """Show system information and status"""
    
    console.print(Panel(
        Text("üìä Ultimate Local AI - System Information", justify="center"),
        style="bold blue",
        border_style="blue"
    ))
    
    # System status
    info_table = Table(show_header=True, header_style="bold blue")
    info_table.add_column("Component", style="cyan", width=25)
    info_table.add_column("Status", style="green", width=15)
    info_table.add_column("Details", style="white", width=40)
    
    # Check Ollama status
    ollama_status = "‚úÖ Running" if check_ollama_running() else "‚ùå Not running"
    ollama_details = f"Available at {OLLAMA_BASE_URL}" if check_ollama_running() else "Start with: ollama serve"
    
    # Check models
    models = get_available_models()
    model_status = f"‚úÖ {len(models)} models" if models else "‚ö†Ô∏è No models"
    model_details = f"Models: {', '.join(models[:2])}{'...' if len(models) > 2 else ''}" if models else "Download with: ollama pull qwen2.5:7b"
    
    system_info = [
        ("ü§ñ Ollama Service", ollama_status, ollama_details),
        ("üìã AI Models", model_status, model_details),
        ("üß† Reasoning Engine", "‚úÖ Active", "Advanced step-by-step problem solving"),
        ("üí¨ Chat Interface", "‚úÖ Active", "Interactive & streaming conversations"),
        ("üé® Rich Interface", "‚úÖ Active", "Beautiful terminal experience"),
        ("üîó GitHub Integration", "‚úÖ Active", "Auto-sync & version control"),
        ("üêç Virtual Environment", "‚úÖ Active", "Python 3.12 isolated environment"),
        ("üîí Privacy", "‚úÖ Maximum", "100% local processing"),
    ]
    
    for component, status, details in system_info:
        info_table.add_row(component, status, details)
    
    console.print(info_table)
    
    # Quick stats
    console.print(f"\n[bold cyan]üìà Quick Stats:[/bold cyan]")
    console.print(f"  ‚Ä¢ Repository: https://github.com/Sairamg18814/ultimate-local-ai")
    console.print(f"  ‚Ä¢ Local Processing: 100% private & secure")
    console.print(f"  ‚Ä¢ Available Models: {len(models)}")
    console.print(f"  ‚Ä¢ Ollama API: {OLLAMA_BASE_URL}")

@app.command()
def serve() -> None:
    """Start Ollama service if not running"""
    
    if check_ollama_running():
        console.print("[green]‚úÖ Ollama is already running[/green]")
        return
    
    console.print("[yellow]üîÑ Starting Ollama service...[/yellow]")
    
    try:
        # Try to start Ollama
        subprocess.run(["ollama", "serve"], check=False)
    except FileNotFoundError:
        console.print("[red]‚ùå Ollama not found. Install with: brew install ollama[/red]")
    except Exception as e:
        console.print(f"[red]‚ùå Error starting Ollama: {e}[/red]")

def main():
    """Main entry point"""
    app()

if __name__ == "__main__":
    main()